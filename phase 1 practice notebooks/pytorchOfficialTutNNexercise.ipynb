{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets, models\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "import os \n",
    "import time\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "rebuild_data = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda: 0\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    \n",
    "#i could assign specific layers to one gpu \n",
    "\n",
    "torch.cuda.device_count() #but i have only one :P\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 3)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 3)\n",
    "        \n",
    "        #16 times 6 times 6 is tricky, \n",
    "        #see: https://pytorch.org/docs/master/generated/torch.nn.Conv2d.html\n",
    "        #apparently keras just does this for you, pytorch should . . . but can't? 'cause of the whole dynamic graph generation thing? which i don't really understand . . . :/\n",
    "        \n",
    "        #EASIER: https://stackoverflow.com/questions/53784998/how-are-the-pytorch-dimensions-for-linear-layers-calculated\n",
    "        \n",
    "        self.fc1 = nn.Linear(16*6*6, 120) #6*6 from image\n",
    "        \n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2,2))\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, self.num_flat_features(x)) \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=576, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = Net()\n",
    "print(net)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[[[ 0.2201,  0.0985, -0.1770],\n",
       "           [ 0.3067, -0.0331, -0.2608],\n",
       "           [-0.1463,  0.0611,  0.0389]]],\n",
       " \n",
       " \n",
       "         [[[-0.2057, -0.2625,  0.2223],\n",
       "           [-0.1649, -0.1147,  0.2816],\n",
       "           [ 0.2135, -0.2841,  0.2121]]],\n",
       " \n",
       " \n",
       "         [[[-0.2673, -0.0658, -0.1994],\n",
       "           [ 0.0227,  0.3152,  0.0683],\n",
       "           [ 0.0493, -0.2714, -0.1086]]],\n",
       " \n",
       " \n",
       "         [[[-0.3091,  0.1030,  0.1806],\n",
       "           [ 0.2342, -0.3220, -0.0632],\n",
       "           [ 0.2490,  0.3095,  0.0663]]],\n",
       " \n",
       " \n",
       "         [[[-0.1216, -0.3290,  0.1539],\n",
       "           [ 0.2009, -0.3059, -0.0697],\n",
       "           [-0.2566,  0.0379, -0.0338]]],\n",
       " \n",
       " \n",
       "         [[[-0.1338,  0.2232, -0.0758],\n",
       "           [ 0.0030, -0.1669, -0.1668],\n",
       "           [ 0.1776,  0.1693,  0.0383]]]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([ 0.1773, -0.2163, -0.0495,  0.2434,  0.0128,  0.0785],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[[[-0.0178,  0.0042,  0.0968],\n",
       "           [-0.0671, -0.1269, -0.0306],\n",
       "           [ 0.0689,  0.0766, -0.0594]],\n",
       " \n",
       "          [[-0.0555,  0.0520, -0.0496],\n",
       "           [ 0.0747,  0.1243, -0.0658],\n",
       "           [ 0.0673, -0.0082, -0.0567]],\n",
       " \n",
       "          [[ 0.0250,  0.1055, -0.0659],\n",
       "           [-0.1328,  0.1317,  0.0121],\n",
       "           [ 0.1003, -0.1032,  0.0530]],\n",
       " \n",
       "          [[-0.0403, -0.0525,  0.0223],\n",
       "           [-0.1004,  0.0673,  0.0548],\n",
       "           [ 0.0430,  0.0113,  0.0326]],\n",
       " \n",
       "          [[ 0.1052,  0.0630,  0.1247],\n",
       "           [ 0.1298,  0.0957,  0.0515],\n",
       "           [-0.1339, -0.0126,  0.1127]],\n",
       " \n",
       "          [[ 0.0222,  0.0797,  0.1114],\n",
       "           [-0.0325,  0.1311, -0.0185],\n",
       "           [-0.1049,  0.1358,  0.1016]]],\n",
       " \n",
       " \n",
       "         [[[ 0.0310,  0.0359,  0.0184],\n",
       "           [ 0.1156,  0.0482, -0.0270],\n",
       "           [ 0.0476, -0.0844,  0.0732]],\n",
       " \n",
       "          [[-0.0748,  0.1103, -0.0579],\n",
       "           [-0.0216,  0.1255,  0.0774],\n",
       "           [ 0.1187,  0.0127, -0.0773]],\n",
       " \n",
       "          [[-0.0688,  0.0219,  0.1097],\n",
       "           [ 0.0497, -0.0287, -0.1046],\n",
       "           [-0.0528,  0.0674,  0.0696]],\n",
       " \n",
       "          [[-0.1086, -0.1306, -0.1267],\n",
       "           [ 0.0595, -0.1320, -0.0021],\n",
       "           [ 0.0288, -0.1056,  0.0318]],\n",
       " \n",
       "          [[ 0.0402, -0.0964, -0.0794],\n",
       "           [ 0.0157,  0.0606,  0.0722],\n",
       "           [ 0.1263, -0.1322, -0.0205]],\n",
       " \n",
       "          [[ 0.0346,  0.1228, -0.0993],\n",
       "           [ 0.1040,  0.1027,  0.0761],\n",
       "           [-0.0982, -0.0125,  0.0577]]],\n",
       " \n",
       " \n",
       "         [[[ 0.0758, -0.0899, -0.1218],\n",
       "           [ 0.0356,  0.0190, -0.1005],\n",
       "           [-0.1323,  0.1157,  0.1027]],\n",
       " \n",
       "          [[ 0.0241, -0.1171, -0.0762],\n",
       "           [ 0.0851, -0.0094, -0.0384],\n",
       "           [-0.1010,  0.1334, -0.1038]],\n",
       " \n",
       "          [[-0.1257, -0.0408, -0.0045],\n",
       "           [ 0.0417, -0.0903,  0.0672],\n",
       "           [ 0.0247, -0.0385,  0.0686]],\n",
       " \n",
       "          [[-0.1346, -0.0932,  0.1113],\n",
       "           [-0.0019,  0.0330, -0.1146],\n",
       "           [-0.0579, -0.0258,  0.0197]],\n",
       " \n",
       "          [[ 0.0679, -0.0600, -0.0912],\n",
       "           [ 0.0757,  0.0979,  0.1156],\n",
       "           [ 0.0809,  0.0991,  0.0930]],\n",
       " \n",
       "          [[ 0.0272,  0.1223, -0.0092],\n",
       "           [ 0.0200, -0.1316,  0.1328],\n",
       "           [-0.0816,  0.0417, -0.1063]]],\n",
       " \n",
       " \n",
       "         [[[ 0.1296,  0.0939, -0.0445],\n",
       "           [ 0.1305,  0.1349, -0.0048],\n",
       "           [ 0.0206, -0.0440,  0.0536]],\n",
       " \n",
       "          [[ 0.1160,  0.1131,  0.0958],\n",
       "           [ 0.0057, -0.1097, -0.0211],\n",
       "           [-0.0888,  0.0289, -0.1060]],\n",
       " \n",
       "          [[ 0.0297, -0.0292,  0.0961],\n",
       "           [ 0.1093, -0.0504,  0.0301],\n",
       "           [-0.1108, -0.0274,  0.0374]],\n",
       " \n",
       "          [[-0.0881, -0.0581,  0.1130],\n",
       "           [ 0.1083, -0.0897, -0.0817],\n",
       "           [ 0.1262,  0.0659,  0.0675]],\n",
       " \n",
       "          [[-0.0545, -0.0557,  0.0379],\n",
       "           [-0.0725,  0.0479,  0.0460],\n",
       "           [ 0.0756,  0.0241,  0.1358]],\n",
       " \n",
       "          [[ 0.0151, -0.1294,  0.1166],\n",
       "           [-0.1094,  0.1259, -0.1018],\n",
       "           [ 0.0233, -0.0494,  0.1284]]],\n",
       " \n",
       " \n",
       "         [[[ 0.0589, -0.0802, -0.0210],\n",
       "           [-0.0256, -0.1357, -0.1043],\n",
       "           [ 0.0044,  0.0537,  0.0224]],\n",
       " \n",
       "          [[-0.0600, -0.0456,  0.0564],\n",
       "           [ 0.1082,  0.1047, -0.0285],\n",
       "           [ 0.1069, -0.1039, -0.1140]],\n",
       " \n",
       "          [[ 0.0156, -0.0113,  0.0498],\n",
       "           [-0.0972, -0.0216,  0.0926],\n",
       "           [ 0.0241, -0.0015, -0.1227]],\n",
       " \n",
       "          [[ 0.0927,  0.0914,  0.0124],\n",
       "           [ 0.0981,  0.1049,  0.0433],\n",
       "           [ 0.1258, -0.0992,  0.1259]],\n",
       " \n",
       "          [[-0.0758, -0.0995,  0.0838],\n",
       "           [ 0.1180,  0.0482, -0.0296],\n",
       "           [-0.0219,  0.0412,  0.0543]],\n",
       " \n",
       "          [[ 0.0331,  0.1126,  0.1329],\n",
       "           [-0.0533,  0.0799, -0.0524],\n",
       "           [-0.0821,  0.0469,  0.0492]]],\n",
       " \n",
       " \n",
       "         [[[ 0.0314,  0.0511, -0.0713],\n",
       "           [ 0.1327, -0.0256, -0.1137],\n",
       "           [ 0.0821, -0.0857,  0.0246]],\n",
       " \n",
       "          [[ 0.0600,  0.1268, -0.0546],\n",
       "           [-0.1250, -0.0207,  0.1201],\n",
       "           [ 0.0616, -0.0789, -0.0284]],\n",
       " \n",
       "          [[ 0.0821,  0.0287,  0.0748],\n",
       "           [-0.0260, -0.0128,  0.0229],\n",
       "           [ 0.0661, -0.0595,  0.1268]],\n",
       " \n",
       "          [[-0.0017,  0.0750, -0.0097],\n",
       "           [ 0.0672, -0.0507,  0.0163],\n",
       "           [-0.0214,  0.0255,  0.0504]],\n",
       " \n",
       "          [[-0.0130, -0.1231,  0.1231],\n",
       "           [-0.0082,  0.0098, -0.0492],\n",
       "           [ 0.0962,  0.0318,  0.0940]],\n",
       " \n",
       "          [[-0.0088, -0.0630,  0.0567],\n",
       "           [ 0.0487, -0.0724, -0.0260],\n",
       "           [-0.0007,  0.0111,  0.1323]]],\n",
       " \n",
       " \n",
       "         [[[-0.0319, -0.1029, -0.1323],\n",
       "           [-0.1129,  0.1173,  0.0805],\n",
       "           [ 0.0846, -0.0507,  0.0288]],\n",
       " \n",
       "          [[-0.0007, -0.1324,  0.0891],\n",
       "           [ 0.0232, -0.0109, -0.1169],\n",
       "           [-0.1099,  0.0134,  0.1007]],\n",
       " \n",
       "          [[ 0.0901, -0.0018, -0.1185],\n",
       "           [-0.0938,  0.0011, -0.1346],\n",
       "           [-0.0071,  0.1151, -0.0445]],\n",
       " \n",
       "          [[ 0.0870, -0.0786,  0.0271],\n",
       "           [ 0.1080,  0.0774,  0.0280],\n",
       "           [ 0.0416, -0.0234,  0.0767]],\n",
       " \n",
       "          [[ 0.0524, -0.0787,  0.0562],\n",
       "           [-0.1143,  0.1278, -0.0655],\n",
       "           [-0.1076, -0.0540, -0.0899]],\n",
       " \n",
       "          [[ 0.0510,  0.0477,  0.0289],\n",
       "           [-0.0279,  0.0064,  0.1037],\n",
       "           [-0.0214,  0.0585,  0.0894]]],\n",
       " \n",
       " \n",
       "         [[[ 0.1090, -0.0672, -0.0961],\n",
       "           [ 0.0225, -0.0663,  0.0762],\n",
       "           [ 0.0806, -0.0534,  0.1225]],\n",
       " \n",
       "          [[-0.0139, -0.0737, -0.0256],\n",
       "           [ 0.0545,  0.0032, -0.1016],\n",
       "           [ 0.1279,  0.0101,  0.0370]],\n",
       " \n",
       "          [[ 0.0802,  0.0743, -0.0612],\n",
       "           [-0.0408,  0.0473, -0.1321],\n",
       "           [-0.0779, -0.1322, -0.1070]],\n",
       " \n",
       "          [[ 0.0282, -0.0864,  0.0618],\n",
       "           [ 0.1286,  0.0997,  0.0927],\n",
       "           [-0.0707, -0.0068, -0.0252]],\n",
       " \n",
       "          [[-0.1131,  0.0476, -0.0280],\n",
       "           [ 0.1090,  0.0219,  0.0026],\n",
       "           [-0.1348,  0.0012, -0.0912]],\n",
       " \n",
       "          [[ 0.0372, -0.0011,  0.0967],\n",
       "           [-0.1174,  0.0684,  0.0097],\n",
       "           [ 0.0923,  0.0910,  0.1092]]],\n",
       " \n",
       " \n",
       "         [[[-0.0277, -0.0122, -0.0542],\n",
       "           [ 0.1036,  0.0174,  0.0093],\n",
       "           [-0.1313, -0.0525, -0.0942]],\n",
       " \n",
       "          [[-0.0221, -0.0611,  0.1100],\n",
       "           [-0.0672, -0.0133,  0.0104],\n",
       "           [ 0.0834,  0.0609,  0.0169]],\n",
       " \n",
       "          [[ 0.0135, -0.1002,  0.1031],\n",
       "           [-0.1085,  0.0207, -0.0302],\n",
       "           [-0.0443, -0.0738, -0.0594]],\n",
       " \n",
       "          [[-0.1273,  0.0464,  0.1304],\n",
       "           [-0.0711,  0.1189, -0.0713],\n",
       "           [ 0.0263, -0.0928, -0.0034]],\n",
       " \n",
       "          [[ 0.0945, -0.1157, -0.0659],\n",
       "           [ 0.0728, -0.0994, -0.1208],\n",
       "           [ 0.0100,  0.0518, -0.1351]],\n",
       " \n",
       "          [[-0.1106, -0.0389, -0.0041],\n",
       "           [-0.1304,  0.0812,  0.1303],\n",
       "           [-0.1143,  0.1243,  0.1209]]],\n",
       " \n",
       " \n",
       "         [[[ 0.0726, -0.0049, -0.0986],\n",
       "           [ 0.1268,  0.0401, -0.1149],\n",
       "           [ 0.0098, -0.1085,  0.0179]],\n",
       " \n",
       "          [[-0.1162,  0.0151,  0.1126],\n",
       "           [-0.0172, -0.1085, -0.0346],\n",
       "           [-0.0932, -0.1238, -0.1226]],\n",
       " \n",
       "          [[ 0.0589, -0.0008, -0.0220],\n",
       "           [ 0.0604,  0.1161,  0.0965],\n",
       "           [ 0.0208, -0.0141, -0.1179]],\n",
       " \n",
       "          [[-0.0114,  0.0832,  0.0052],\n",
       "           [ 0.0324, -0.0216,  0.0701],\n",
       "           [ 0.0339, -0.0025,  0.0871]],\n",
       " \n",
       "          [[-0.1269, -0.1163, -0.1274],\n",
       "           [-0.1078, -0.0895,  0.0811],\n",
       "           [ 0.0021, -0.0957, -0.0552]],\n",
       " \n",
       "          [[-0.0050,  0.0403, -0.0096],\n",
       "           [-0.1318,  0.1238, -0.1248],\n",
       "           [ 0.1198, -0.0720,  0.1122]]],\n",
       " \n",
       " \n",
       "         [[[ 0.1298, -0.1238,  0.0026],\n",
       "           [-0.0407,  0.1298,  0.1212],\n",
       "           [ 0.0152,  0.0494,  0.0485]],\n",
       " \n",
       "          [[ 0.0128,  0.0965,  0.0142],\n",
       "           [ 0.0923,  0.0060,  0.0474],\n",
       "           [ 0.1185,  0.1287, -0.0492]],\n",
       " \n",
       "          [[ 0.0252,  0.0515, -0.0751],\n",
       "           [ 0.0997,  0.0720, -0.0475],\n",
       "           [ 0.1312, -0.0729, -0.1300]],\n",
       " \n",
       "          [[ 0.0154, -0.0507,  0.1352],\n",
       "           [ 0.0938, -0.1284,  0.1014],\n",
       "           [ 0.0939,  0.1148,  0.1190]],\n",
       " \n",
       "          [[ 0.1318, -0.0323,  0.1221],\n",
       "           [ 0.0416,  0.1241, -0.0003],\n",
       "           [-0.0253,  0.0407, -0.0751]],\n",
       " \n",
       "          [[ 0.1058,  0.0737, -0.0050],\n",
       "           [-0.0590, -0.0247,  0.0132],\n",
       "           [-0.1319, -0.0250,  0.0317]]],\n",
       " \n",
       " \n",
       "         [[[ 0.0437,  0.0965, -0.0496],\n",
       "           [-0.1294,  0.0342,  0.0288],\n",
       "           [-0.0163, -0.1023,  0.1261]],\n",
       " \n",
       "          [[ 0.0961, -0.0421,  0.0843],\n",
       "           [-0.0102,  0.0369, -0.1282],\n",
       "           [ 0.1208, -0.0806, -0.0867]],\n",
       " \n",
       "          [[-0.0739,  0.0859,  0.1125],\n",
       "           [ 0.1018, -0.0073,  0.0654],\n",
       "           [-0.1254,  0.0777, -0.0221]],\n",
       " \n",
       "          [[-0.0477,  0.0853, -0.0404],\n",
       "           [ 0.1203, -0.0988,  0.0354],\n",
       "           [-0.1260,  0.0227,  0.0392]],\n",
       " \n",
       "          [[-0.0058, -0.0042,  0.0455],\n",
       "           [-0.0992,  0.0665, -0.0352],\n",
       "           [ 0.0315,  0.0200,  0.0743]],\n",
       " \n",
       "          [[-0.0658,  0.1097,  0.0905],\n",
       "           [-0.0426,  0.0421, -0.0796],\n",
       "           [ 0.0219,  0.0435, -0.0097]]],\n",
       " \n",
       " \n",
       "         [[[ 0.0331, -0.0389, -0.0166],\n",
       "           [ 0.1098, -0.0872,  0.1058],\n",
       "           [ 0.0381,  0.1302, -0.1162]],\n",
       " \n",
       "          [[ 0.0664,  0.0756, -0.1320],\n",
       "           [ 0.1089, -0.1185,  0.1300],\n",
       "           [ 0.1209, -0.0628, -0.0887]],\n",
       " \n",
       "          [[ 0.0400,  0.0964, -0.0736],\n",
       "           [ 0.0426,  0.0618, -0.0044],\n",
       "           [ 0.0438, -0.0890, -0.0022]],\n",
       " \n",
       "          [[ 0.1206, -0.0355,  0.1203],\n",
       "           [ 0.0304,  0.0505,  0.0565],\n",
       "           [ 0.0779,  0.0744, -0.0099]],\n",
       " \n",
       "          [[ 0.0288, -0.0483,  0.0925],\n",
       "           [-0.1330,  0.1027, -0.1168],\n",
       "           [ 0.1017,  0.0344, -0.1069]],\n",
       " \n",
       "          [[-0.0439,  0.0148,  0.0884],\n",
       "           [ 0.0064, -0.0764,  0.1179],\n",
       "           [ 0.0079,  0.0856, -0.0533]]],\n",
       " \n",
       " \n",
       "         [[[ 0.0750, -0.0228, -0.0410],\n",
       "           [ 0.1024, -0.0879, -0.0491],\n",
       "           [ 0.0909,  0.0531,  0.0611]],\n",
       " \n",
       "          [[ 0.0982,  0.0037, -0.0818],\n",
       "           [-0.1184, -0.0957,  0.0273],\n",
       "           [-0.0232,  0.1011,  0.0989]],\n",
       " \n",
       "          [[ 0.1219,  0.0052, -0.0077],\n",
       "           [ 0.0423, -0.0025,  0.1284],\n",
       "           [ 0.1032,  0.0852,  0.0672]],\n",
       " \n",
       "          [[ 0.0580,  0.0911, -0.1006],\n",
       "           [ 0.1249,  0.0930, -0.0663],\n",
       "           [-0.0522, -0.0490, -0.0349]],\n",
       " \n",
       "          [[-0.0316, -0.0873, -0.1018],\n",
       "           [-0.0510, -0.1053,  0.1272],\n",
       "           [-0.1247,  0.0395,  0.0638]],\n",
       " \n",
       "          [[-0.0573,  0.0622,  0.1360],\n",
       "           [-0.1025, -0.0717,  0.0035],\n",
       "           [-0.0151,  0.0827, -0.1308]]],\n",
       " \n",
       " \n",
       "         [[[-0.0512,  0.0887,  0.0054],\n",
       "           [-0.0952, -0.1252,  0.1086],\n",
       "           [-0.1190,  0.0053, -0.0339]],\n",
       " \n",
       "          [[ 0.0438, -0.0013,  0.0238],\n",
       "           [ 0.0745, -0.0690, -0.0471],\n",
       "           [ 0.0569, -0.1304, -0.1285]],\n",
       " \n",
       "          [[ 0.1300, -0.0743, -0.0602],\n",
       "           [ 0.0497,  0.1319,  0.0820],\n",
       "           [-0.0407,  0.0152, -0.1118]],\n",
       " \n",
       "          [[-0.0477,  0.0771, -0.0682],\n",
       "           [ 0.0492,  0.0592, -0.0922],\n",
       "           [ 0.1101,  0.1002,  0.1144]],\n",
       " \n",
       "          [[ 0.0506,  0.0784, -0.0508],\n",
       "           [ 0.0428,  0.0540, -0.1264],\n",
       "           [-0.0361, -0.1309,  0.0823]],\n",
       " \n",
       "          [[ 0.1041, -0.0026, -0.0156],\n",
       "           [-0.0163,  0.0365,  0.1053],\n",
       "           [-0.1062,  0.0817, -0.0719]]],\n",
       " \n",
       " \n",
       "         [[[ 0.1311, -0.0100, -0.0885],\n",
       "           [-0.1294,  0.0258, -0.0153],\n",
       "           [-0.0630,  0.0738,  0.0853]],\n",
       " \n",
       "          [[ 0.1182,  0.0657, -0.1158],\n",
       "           [-0.0941,  0.0893,  0.0629],\n",
       "           [-0.0876, -0.0644, -0.0824]],\n",
       " \n",
       "          [[-0.1313,  0.1298,  0.0799],\n",
       "           [-0.0577,  0.0484, -0.1103],\n",
       "           [ 0.0096, -0.0822,  0.0208]],\n",
       " \n",
       "          [[ 0.0591, -0.1029,  0.0626],\n",
       "           [-0.0310,  0.0920,  0.0810],\n",
       "           [ 0.0456,  0.1070, -0.1019]],\n",
       " \n",
       "          [[ 0.0192, -0.1145, -0.1151],\n",
       "           [ 0.1223,  0.0864,  0.1355],\n",
       "           [ 0.0915, -0.1138,  0.0407]],\n",
       " \n",
       "          [[ 0.1332,  0.1230, -0.0897],\n",
       "           [ 0.0166,  0.1238, -0.0898],\n",
       "           [-0.0894, -0.0678,  0.0111]]]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([ 0.0849, -0.0935,  0.1067, -0.0900,  0.0960, -0.0328, -0.0704,  0.1033,\n",
       "          0.0323,  0.0736,  0.0067, -0.0440, -0.0136,  0.1090,  0.0339, -0.0849],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.0340, -0.0413, -0.0117,  ...,  0.0187,  0.0154, -0.0087],\n",
       "         [-0.0303, -0.0373,  0.0101,  ...,  0.0384, -0.0403, -0.0343],\n",
       "         [ 0.0343, -0.0355,  0.0142,  ..., -0.0338,  0.0287,  0.0080],\n",
       "         ...,\n",
       "         [-0.0146, -0.0144, -0.0359,  ...,  0.0337, -0.0403, -0.0007],\n",
       "         [-0.0156,  0.0010,  0.0029,  ...,  0.0021,  0.0314, -0.0377],\n",
       "         [-0.0012,  0.0032,  0.0144,  ..., -0.0016,  0.0006, -0.0286]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([ 0.0312, -0.0013, -0.0265, -0.0191,  0.0037,  0.0102, -0.0137,  0.0351,\n",
       "         -0.0342,  0.0329, -0.0155, -0.0026, -0.0246, -0.0413, -0.0001,  0.0321,\n",
       "         -0.0232, -0.0167, -0.0208,  0.0093,  0.0278,  0.0340,  0.0165, -0.0124,\n",
       "          0.0236,  0.0215, -0.0392,  0.0199,  0.0078,  0.0276,  0.0315,  0.0146,\n",
       "         -0.0316,  0.0223,  0.0277,  0.0083,  0.0022,  0.0033, -0.0110, -0.0227,\n",
       "          0.0391, -0.0175,  0.0408,  0.0102,  0.0002, -0.0081, -0.0283, -0.0091,\n",
       "          0.0213, -0.0287, -0.0214,  0.0034,  0.0175, -0.0270, -0.0083, -0.0314,\n",
       "         -0.0030,  0.0128,  0.0150,  0.0302,  0.0414,  0.0415, -0.0102,  0.0053,\n",
       "          0.0095,  0.0351,  0.0218, -0.0100,  0.0148, -0.0213,  0.0338,  0.0232,\n",
       "          0.0154, -0.0353,  0.0270,  0.0075,  0.0145,  0.0173, -0.0261, -0.0328,\n",
       "         -0.0383, -0.0144,  0.0099, -0.0156,  0.0328,  0.0318,  0.0387,  0.0158,\n",
       "         -0.0285,  0.0416, -0.0349,  0.0212,  0.0342,  0.0034,  0.0128,  0.0239,\n",
       "          0.0167,  0.0215,  0.0404, -0.0113,  0.0201, -0.0069,  0.0083,  0.0030,\n",
       "          0.0292, -0.0340,  0.0030, -0.0121,  0.0028,  0.0026,  0.0208,  0.0240,\n",
       "         -0.0319,  0.0320,  0.0037,  0.0214, -0.0360,  0.0280,  0.0390,  0.0354],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.0131, -0.0798, -0.0123,  ..., -0.0639,  0.0447, -0.0449],\n",
       "         [-0.0494, -0.0423, -0.0603,  ...,  0.0534, -0.0415,  0.0029],\n",
       "         [-0.0516, -0.0848,  0.0701,  ..., -0.0653, -0.0896,  0.0415],\n",
       "         ...,\n",
       "         [ 0.0506,  0.0712,  0.0043,  ...,  0.0505, -0.0234,  0.0431],\n",
       "         [ 0.0581,  0.0075,  0.0720,  ..., -0.0877, -0.0360, -0.0382],\n",
       "         [ 0.0060, -0.0371, -0.0841,  ...,  0.0168, -0.0549, -0.0358]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([ 0.0094,  0.0778, -0.0007,  0.0860, -0.0318, -0.0043,  0.0486,  0.0357,\n",
       "         -0.0720,  0.0396,  0.0537,  0.0191, -0.0641, -0.0149, -0.0436,  0.0725,\n",
       "         -0.0212,  0.0225, -0.0509, -0.0587,  0.0727,  0.0267, -0.0737,  0.0664,\n",
       "          0.0548, -0.0353, -0.0446, -0.0402, -0.0013, -0.0889, -0.0546, -0.0390,\n",
       "         -0.0168,  0.0246,  0.0672, -0.0784, -0.0540,  0.0261,  0.0083, -0.0803,\n",
       "         -0.0363,  0.0910,  0.0581,  0.0303,  0.0890, -0.0042, -0.0119,  0.0746,\n",
       "         -0.0762, -0.0872,  0.0612, -0.0613,  0.0552, -0.0396, -0.0561,  0.0624,\n",
       "          0.0641, -0.0523, -0.0039, -0.0639, -0.0733, -0.0075,  0.0056,  0.0017,\n",
       "          0.0506, -0.0298, -0.0729,  0.0138, -0.0543, -0.0756, -0.0697,  0.0889,\n",
       "         -0.0697, -0.0272,  0.0279, -0.0862,  0.0078, -0.0483,  0.0026,  0.0610,\n",
       "         -0.0362, -0.0135, -0.0167, -0.0683], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.0635, -0.0778,  0.1018, -0.0167, -0.0941, -0.0526,  0.0619, -0.0369,\n",
       "          -0.0262,  0.0478,  0.0658, -0.0244, -0.0096,  0.0651,  0.0240, -0.0225,\n",
       "           0.0815, -0.0251,  0.0823,  0.0730,  0.0975,  0.0270, -0.0845,  0.0917,\n",
       "          -0.1031, -0.0772,  0.1010, -0.1073,  0.0911, -0.0153,  0.0402, -0.0997,\n",
       "           0.0690,  0.0325,  0.0785,  0.0393,  0.0607, -0.0290, -0.0364, -0.0885,\n",
       "          -0.0940,  0.0925,  0.0875, -0.0991, -0.0114,  0.0508, -0.0912, -0.0895,\n",
       "           0.0658,  0.0774, -0.1074, -0.1015, -0.0100,  0.0030, -0.0086, -0.0407,\n",
       "          -0.1003, -0.0943,  0.0994,  0.0820, -0.0186,  0.0758, -0.0923, -0.0012,\n",
       "           0.0313,  0.0920,  0.0195, -0.0198,  0.0498,  0.0996,  0.0611, -0.0227,\n",
       "          -0.0131, -0.0482, -0.0050, -0.0522,  0.0159,  0.0405,  0.0591, -0.0954,\n",
       "           0.0979,  0.0279, -0.1089,  0.0494],\n",
       "         [-0.0946,  0.0025,  0.0371,  0.0149,  0.0573, -0.0926, -0.0862, -0.0647,\n",
       "          -0.0904,  0.0049, -0.0398, -0.0533,  0.0323,  0.0049, -0.0812,  0.0771,\n",
       "           0.0253,  0.0122, -0.0997, -0.0642,  0.0524, -0.0681, -0.0556,  0.0460,\n",
       "           0.0353,  0.0575, -0.0866,  0.0313,  0.0349, -0.0006,  0.0927, -0.0321,\n",
       "           0.0960, -0.0777,  0.1061, -0.0423, -0.0952, -0.0319, -0.0772, -0.0257,\n",
       "          -0.0065, -0.0912,  0.0682,  0.0221, -0.0333,  0.0397,  0.1022,  0.0304,\n",
       "          -0.0618,  0.0050,  0.0842, -0.0381, -0.0288,  0.0393, -0.0365,  0.1016,\n",
       "          -0.0512, -0.0260, -0.0172, -0.0647, -0.0028, -0.1032, -0.0287,  0.0466,\n",
       "          -0.0830, -0.0730,  0.0605,  0.1070, -0.0454, -0.0760, -0.0053, -0.0491,\n",
       "          -0.0772,  0.0300, -0.0085, -0.0035, -0.0935,  0.0652, -0.0942, -0.0477,\n",
       "           0.0826,  0.0153,  0.0817,  0.0029],\n",
       "         [-0.1001,  0.1013, -0.0897, -0.0305, -0.0158, -0.0552, -0.1091,  0.0314,\n",
       "           0.0815, -0.0847,  0.0386, -0.0137,  0.0161,  0.0312, -0.0715,  0.0106,\n",
       "           0.0180,  0.0441,  0.0263, -0.0453, -0.0611, -0.0059,  0.0949, -0.0348,\n",
       "          -0.0055, -0.0741, -0.0235,  0.0646, -0.0929,  0.1039, -0.0927,  0.0372,\n",
       "          -0.0314,  0.0432,  0.0006, -0.0035, -0.0565, -0.0451,  0.0119, -0.0510,\n",
       "           0.0919,  0.0611,  0.0365, -0.0297,  0.0981, -0.0983,  0.0202,  0.0410,\n",
       "          -0.1022,  0.0318, -0.1078,  0.0212,  0.0057, -0.0497,  0.0641,  0.0515,\n",
       "           0.0544,  0.0090,  0.1003, -0.0948, -0.0932, -0.0539,  0.0997, -0.0004,\n",
       "          -0.0719,  0.1084,  0.0438, -0.0704,  0.0681, -0.0284, -0.0158, -0.0196,\n",
       "          -0.0268,  0.0229, -0.0792,  0.0513, -0.0232,  0.1061,  0.0517, -0.0466,\n",
       "           0.0875,  0.0026, -0.1066,  0.0309],\n",
       "         [-0.0462,  0.0818,  0.0554, -0.0718, -0.0861,  0.0806,  0.0521,  0.0361,\n",
       "           0.0634, -0.0542,  0.0501,  0.0918, -0.0587, -0.0111,  0.0365,  0.0163,\n",
       "           0.0174, -0.0793,  0.0576, -0.0902,  0.0747,  0.0328,  0.0403,  0.0731,\n",
       "           0.0311, -0.0804, -0.0825, -0.0965, -0.0617,  0.0649,  0.0904,  0.0288,\n",
       "          -0.0399,  0.1016,  0.0261,  0.0452, -0.0798, -0.0970, -0.0274, -0.0418,\n",
       "          -0.1002, -0.0632, -0.0812,  0.1077, -0.0127,  0.0693, -0.0400, -0.0365,\n",
       "          -0.0632, -0.0320, -0.0516,  0.0546,  0.0839,  0.0382, -0.0349,  0.0017,\n",
       "          -0.0314,  0.0006,  0.0017, -0.0077,  0.0875,  0.0439, -0.0990,  0.0101,\n",
       "           0.0346,  0.0664, -0.0824,  0.0614, -0.0461,  0.0227, -0.0055, -0.0217,\n",
       "           0.0072, -0.1014, -0.0411,  0.0520, -0.0761,  0.0739, -0.1030, -0.0974,\n",
       "          -0.0466, -0.0293, -0.0816, -0.0304],\n",
       "         [ 0.0996, -0.0003,  0.0428,  0.0826,  0.1017,  0.0512, -0.0711,  0.0016,\n",
       "          -0.0919, -0.0439,  0.0006,  0.0932,  0.1012,  0.0845,  0.0176,  0.0834,\n",
       "          -0.1019,  0.0266,  0.0796,  0.0138,  0.0946,  0.0570,  0.0138,  0.0931,\n",
       "           0.0021, -0.0101, -0.0215,  0.0397,  0.0176,  0.0835,  0.0117, -0.0205,\n",
       "           0.0043,  0.0031,  0.0539, -0.0694, -0.0468,  0.0636,  0.0415,  0.0461,\n",
       "           0.0925, -0.0976, -0.0042, -0.1044, -0.0330,  0.0686,  0.0299,  0.0181,\n",
       "          -0.0851,  0.0878,  0.0706,  0.0420,  0.0746, -0.1005, -0.1091, -0.0388,\n",
       "           0.0687, -0.1000,  0.1016, -0.0604, -0.0204,  0.0391, -0.1038, -0.0781,\n",
       "           0.0056, -0.1051,  0.0111, -0.1014,  0.0357, -0.0588, -0.0152,  0.0893,\n",
       "          -0.0133,  0.0818, -0.0967, -0.0787,  0.0277, -0.0886, -0.0902,  0.0453,\n",
       "           0.0601,  0.0696, -0.0750,  0.0119],\n",
       "         [ 0.1008, -0.1048, -0.0948, -0.0110, -0.1036,  0.0099,  0.0435, -0.0722,\n",
       "          -0.0515, -0.0588,  0.0593, -0.0115,  0.0455, -0.0680, -0.0247, -0.1039,\n",
       "          -0.0533,  0.0864, -0.0820, -0.0189,  0.0652, -0.0856, -0.0238, -0.0934,\n",
       "          -0.0164,  0.0082,  0.0570,  0.0806,  0.0273, -0.0726, -0.0386, -0.0224,\n",
       "           0.0263, -0.0971,  0.0894, -0.0722, -0.0625,  0.0649,  0.0864,  0.0712,\n",
       "          -0.0235, -0.0552,  0.0437, -0.0992, -0.0495, -0.0913,  0.0428,  0.0496,\n",
       "          -0.0608,  0.0713, -0.0535, -0.0771, -0.0630, -0.0603, -0.0057,  0.0092,\n",
       "           0.0089,  0.0600,  0.1071,  0.0597, -0.0784, -0.0866, -0.0298, -0.0343,\n",
       "          -0.0986, -0.0303, -0.1046,  0.0667, -0.0142, -0.0456,  0.0892, -0.0808,\n",
       "           0.0788,  0.0756, -0.0864, -0.0476,  0.0159,  0.0664, -0.0380, -0.0869,\n",
       "           0.0759,  0.0332,  0.1014,  0.0141],\n",
       "         [-0.0135,  0.0776,  0.0002,  0.0728,  0.0725,  0.0529, -0.0443, -0.0103,\n",
       "           0.0420, -0.0071,  0.1051,  0.0239, -0.0837,  0.0311,  0.0129,  0.0354,\n",
       "          -0.0123,  0.0522,  0.0558,  0.0609, -0.0819, -0.1068,  0.0744, -0.0656,\n",
       "          -0.0524,  0.0507,  0.0909,  0.0775,  0.0617, -0.0404, -0.0627, -0.0754,\n",
       "           0.0566,  0.0251,  0.0337,  0.0142, -0.1046, -0.0110,  0.0507, -0.0433,\n",
       "          -0.0473, -0.0839, -0.0538,  0.0611, -0.0788, -0.0284,  0.0548, -0.0775,\n",
       "           0.0230,  0.0497,  0.0191, -0.0362,  0.0530, -0.0468, -0.1053,  0.0181,\n",
       "           0.0416, -0.1035, -0.0784, -0.0144, -0.0582,  0.0641,  0.0669, -0.0253,\n",
       "          -0.0888, -0.0170,  0.0462,  0.0971, -0.0567, -0.0279, -0.0006, -0.0535,\n",
       "          -0.0190,  0.1058,  0.0610,  0.0347, -0.0818, -0.0279, -0.0284,  0.0118,\n",
       "          -0.0445,  0.0424,  0.0922, -0.0150],\n",
       "         [-0.0255, -0.0065,  0.0040,  0.0659, -0.0285, -0.0913,  0.0090, -0.0124,\n",
       "          -0.0587, -0.0138,  0.0073,  0.0774,  0.0857, -0.0092, -0.0364, -0.0823,\n",
       "           0.0599,  0.0390,  0.1034,  0.0884,  0.0969,  0.0929, -0.0939, -0.1073,\n",
       "           0.0461, -0.0987, -0.0221, -0.0286,  0.0625,  0.0389,  0.0686, -0.0044,\n",
       "          -0.0938, -0.0279, -0.0805, -0.0215,  0.0741,  0.0973,  0.0615, -0.0405,\n",
       "           0.0287,  0.0521, -0.0334,  0.0050, -0.0735, -0.0816, -0.0804, -0.0232,\n",
       "          -0.0986,  0.0950, -0.0606, -0.0363,  0.0674, -0.0479, -0.0748,  0.0603,\n",
       "          -0.0903,  0.0766, -0.0433,  0.0862, -0.1046,  0.0470,  0.0461, -0.0523,\n",
       "           0.0138,  0.0599, -0.0895, -0.0300,  0.0881, -0.0459, -0.0114,  0.1028,\n",
       "           0.1011,  0.0095,  0.0140, -0.0569, -0.0640, -0.1080, -0.0216,  0.0535,\n",
       "          -0.0946, -0.0476,  0.0352,  0.0583],\n",
       "         [ 0.0999,  0.0918,  0.0687,  0.0254,  0.0133,  0.0329, -0.0130,  0.1065,\n",
       "          -0.0339, -0.0664,  0.0441,  0.0411, -0.0180,  0.0211,  0.0213,  0.0188,\n",
       "           0.0245, -0.0084,  0.0400,  0.0566, -0.0550, -0.0068,  0.0478,  0.1065,\n",
       "           0.0455,  0.0225,  0.0718, -0.0655,  0.0261,  0.0368,  0.0036,  0.0438,\n",
       "          -0.0693,  0.0641,  0.0483,  0.0754, -0.0851,  0.0804,  0.0291,  0.0977,\n",
       "          -0.0163,  0.0120, -0.0678,  0.1031,  0.0252,  0.0212, -0.0186, -0.0590,\n",
       "           0.0462,  0.0791, -0.0528,  0.0328, -0.0736, -0.0555, -0.0216,  0.0383,\n",
       "           0.0447,  0.0258, -0.0180,  0.1037, -0.0874, -0.0933, -0.0483,  0.0869,\n",
       "           0.0072, -0.0218,  0.0916,  0.0692, -0.0442, -0.0327,  0.0724, -0.0531,\n",
       "           0.0282,  0.0793, -0.0226, -0.0166, -0.0415, -0.0464, -0.0491,  0.1060,\n",
       "           0.0034, -0.0309,  0.0711,  0.0691],\n",
       "         [ 0.0376,  0.0577, -0.0279,  0.0151,  0.0417, -0.0901, -0.0627,  0.0101,\n",
       "           0.1021, -0.0519,  0.0426, -0.0196,  0.0069, -0.0449,  0.0801, -0.1047,\n",
       "          -0.1021, -0.0354,  0.0030, -0.0598, -0.0281, -0.0346,  0.0904, -0.0770,\n",
       "           0.0840, -0.1065,  0.0224, -0.0637, -0.0429, -0.0941,  0.0722, -0.0024,\n",
       "          -0.0355,  0.0126, -0.0682, -0.0256,  0.0441,  0.1035,  0.0435,  0.0716,\n",
       "          -0.0758,  0.0980,  0.0717,  0.0027,  0.0713,  0.0495, -0.0757,  0.0995,\n",
       "           0.0401,  0.0840,  0.0905,  0.0461,  0.0882, -0.0371,  0.0278,  0.0078,\n",
       "           0.0465, -0.0163,  0.0282,  0.0424, -0.0518, -0.1083, -0.0194,  0.0362,\n",
       "           0.0688,  0.0510, -0.1070, -0.0486,  0.0499,  0.0749,  0.0269,  0.0483,\n",
       "          -0.0136, -0.1026,  0.0505, -0.0351,  0.0046, -0.1050,  0.0356, -0.1006,\n",
       "          -0.0858,  0.0034, -0.0169, -0.0601]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([ 0.1086, -0.0181,  0.0251,  0.0148,  0.0875, -0.0646, -0.0737,  0.0680,\n",
       "          0.0517,  0.0192], requires_grad=True)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(net.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 1, 3, 3])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = list(net.parameters())\n",
    "params[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1113, -0.0649, -0.0035, -0.0635,  0.1246, -0.0999, -0.0174,  0.1339,\n",
      "          0.1270,  0.0493]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "input = torch.randn(1, 1, 32, 32)\n",
    "out = net(input)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.backward(torch.randn(1, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1113, -0.0649, -0.0035, -0.0635,  0.1246, -0.0999, -0.0174,  0.1339,\n",
      "          0.1270,  0.0493]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Note\n",
    "\n",
    "torch.nn only supports mini-batches. The entire torch.nn package only supports inputs that are a mini-batch of samples, and not a single sample.\n",
    "\n",
    "For example, nn.Conv2d will take in a 4D Tensor of nSamples x nChannels x Height x Width.\n",
    "\n",
    "If you have a single sample, just use input.unsqueeze(0) to add a fake batch dimension.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4248, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "output = net(input)\n",
    "target = torch.randn(10)  # a dummy target, for example\n",
    "target = target.view(1, -1)  # make it the same shape as output\n",
    "\n",
    "\n",
    "#loss = nn.MSELoss(output, target) doesn't work?\n",
    "lossfn = nn.MSELoss()\n",
    "loss = lossfn(output, target)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Now, if you follow loss in the backward direction, using its .grad_fn attribute, you will see a graph of computations that looks like this:\n",
    "\n",
    "input -> conv2d -> relu -> maxpool2d -> conv2d -> relu -> maxpool2d\n",
    "      -> view -> linear -> relu -> linear -> relu -> linear\n",
    "      -> MSELoss\n",
    "      -> loss\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<MseLossBackward object at 0x0000013DD67D5A88>\n",
      "<AddmmBackward object at 0x0000013DD67D5988>\n",
      "<AccumulateGrad object at 0x0000013DD67D5A88>\n"
     ]
    }
   ],
   "source": [
    "print(loss.grad_fn)  # MSELoss\n",
    "print(loss.grad_fn.next_functions[0][0])  # Linear\n",
    "print(loss.grad_fn.next_functions[0][0].next_functions[0][0])  # ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.bias.grad before backward\n",
      "tensor([0., 0., 0., 0., 0., 0.])\n",
      "conv1.bias.grad after backward\n",
      "tensor([ 0.0151,  0.0065, -0.0062, -0.0079,  0.0113,  0.0068])\n"
     ]
    }
   ],
   "source": [
    "net.zero_grad()     # zeroes the gradient buffers of all parameters\n",
    "\n",
    "print('conv1.bias.grad before backward')\n",
    "print(net.conv1.bias.grad)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print('conv1.bias.grad after backward')\n",
    "print(net.conv1.bias.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can implement BACKPROP aka updating the weights according to grads \n",
    "#calculated (during loss.backward, they're stored in the \n",
    "#Tensor objects) using simple Python code:\n",
    "\n",
    "learning_rate = 0.01\n",
    "for f in net.parameters():\n",
    "    f.data.sub_(f.grad.data * learning_rate)\n",
    "\n",
    "#but there's optimisers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "\n",
    "# in your training loop:\n",
    "optimizer.zero_grad()   # zero the gradient buffers\n",
    "#net.zero_grad() #ALSO WORKS the prev is better in context of different optimisers assigned to different layers\n",
    "output = net(input)\n",
    "loss = lossfn(output, target)\n",
    "loss.backward()\n",
    "optimizer.step()    # Does the update\n",
    "\n",
    "#GRADIENTS ARE ACCUMULATED (aka summed?), when backward is called\n",
    "#Observe how gradient buffers had to be manually set to zero using optimizer.zero_grad(). This is because gradients are accumulated "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
